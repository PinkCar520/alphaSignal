import time
import json
import asyncio
from datetime import datetime
import pytz
from src.alphasignal.config import settings
from src.alphasignal.core.logger import logger

# å¼•å…¥ç»„ä»¶
from src.alphasignal.providers.data_sources.google_news import GoogleNewsSource
from src.alphasignal.providers.data_sources.rsshub import RSSHubSource
from src.alphasignal.providers.llm.gemini import GeminiLLM
from src.alphasignal.providers.llm.deepseek import DeepSeekLLM
from src.alphasignal.providers.channels.email import EmailChannel
from src.alphasignal.providers.channels.bark import BarkChannel
from src.alphasignal.core.database import IntelligenceDB
from src.alphasignal.core.backtest import BacktestEngine
from src.alphasignal.core.deduplication import NewsDeduplicator

class AlphaEngine:
    def __init__(self):
        self.sources = [
            GoogleNewsSource(),
            RSSHubSource()
        ]
        self.primary_llm = GeminiLLM()
        self.fallback_llm = DeepSeekLLM()
        self.channels = [EmailChannel(), BarkChannel()]
        self.db = IntelligenceDB()
        self.backtester = BacktestEngine(self.db)
        self.deduplicator = NewsDeduplicator()
        
        # Concurrency Control
        self.ai_semaphore = asyncio.Semaphore(5) # Limit to 5 concurrent AI calls
        
        # Bootstrap deduplicator history from DB
        self._bootstrap_deduplicator()
        
    def _bootstrap_deduplicator(self):
        """Load recent intelligence from DB to initialize deduplicator history"""
        logger.info("ğŸ§µæ­£åœ¨åˆå§‹åŒ–å»é‡å¼•æ“å†å²æ•°æ®...")
        try:
            # Load intelligence from the last 24 hours
            recent_items = self.db.get_recent_intelligence(limit=200)
            if recent_items:
                # Process from oldest to newest to maintain correct history order
                for item in reversed(recent_items):
                    # Combine summary and content for consistent matching
                    summary = item.get('summary')
                    summary_text = ""
                    if isinstance(summary, dict):
                        summary_text = summary.get('en', '') or str(summary)
                    elif isinstance(summary, str):
                        summary_text = summary
                    
                    text = summary_text if len(summary_text) > 20 else (item.get('content') or "")
                    if text:
                        clean_text = self.deduplicator.normalize(text)
                        if clean_text:
                            from simhash import Simhash
                            sh = Simhash(clean_text)
                            
                            # --- Persistent Embedding Cache Logic ---
                            vec = None
                            db_vec_binary = item.get('embedding')
                            
                            if db_vec_binary:
                                try:
                                    import pickle
                                    vec = pickle.loads(db_vec_binary)
                                except Exception as e:
                                    logger.warning(f"Failed to deserialize embedding for ID {item.get('id')}: {e}")
                            
                            # Fallback if no cache in DB
                            if vec is None and self.deduplicator.model:
                                try:
                                    vec = self.deduplicator.model.encode(clean_text)
                                except Exception as e:
                                    logger.warning(f"Semantic encoding failed during bootstrap: {e}")
                            
                            self.deduplicator.add_to_history(sh, vec, record_id=item.get('id'))
                
                logger.info(f"âœ… å·²åŠ è½½ {len(self.deduplicator.simhash_history)} æ¡è®°å½•åˆ°å»é‡å¼•æ“å†å²ã€‚")
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å»é‡å¼•æ“å¤±è´¥: {e}")

    async def run_once_async(self):
        """
        æ ¸å¿ƒå¼‚æ­¥æµæ°´çº¿ï¼š
        1. å‘ç°æ–°æƒ…æŠ¥ (Discovery)
        2. å…¨æ–‡æŠ“å– (Enrichment)
        3. çŠ¶æ€æ£€æŸ¥ä¸è¡¥è¯¾ (Reconciliation)
        4. å¹¶è¡Œ AI åˆ†æ (Analysis)
        """
        logger.info(">>> å¯åŠ¨æµå¼æƒ…æŠ¥æ‰«æ...")
        
        # 0. å¼‚æ­¥åŒæ­¥æ”¶ç›Šç‡
        await asyncio.to_thread(self.backtester.sync_outcomes)

        # 1. åŒæ­¥å‘ç°æ–°æƒ…æŠ¥ (Discovery Phase)
        discovered_items = []
        for source in self.sources:
            try:
                items = await asyncio.to_thread(source.fetch)
                if items:
                    discovered_items.extend(items if isinstance(items, list) else [items])
            except Exception as e:
                logger.error(f"æ•°æ®æºå‘ç°å¼‚å¸¸: {e}")

        # 2. åˆå§‹å…¥åº“å¹¶æ ‡è®°ä¸º PENDING
        for item in discovered_items:
            await asyncio.to_thread(self.db.save_raw_intelligence, item)

        # 3. è¡¥è¯¾æœºåˆ¶ï¼šè·å–æ‰€æœ‰æœªå®Œæˆåˆ†æçš„è®°å½• (PENDING/FAILED)
        pending_records = await asyncio.to_thread(self.db.get_pending_intelligence, limit=20)
        
        if not pending_records:
            logger.info("æ— å¾…åˆ†ææƒ…æŠ¥ï¼Œæœ¬è½®ç»“æŸã€‚")
            return

        # 4. æ·±åº¦æå–å…¨æ–‡ (åªé’ˆå¯¹å¾…åˆ†æçš„)
        from src.alphasignal.utils.crawler import AsyncRichCrawler
        crawler = AsyncRichCrawler()
        enriched_items = await crawler.batch_crawl(pending_records)

        # 5. å¹¶è¡Œå¹¶å‘ AI åˆ†æ
        logger.info(f"ğŸš€ å¹¶è¡Œåˆ†æä¸­ (å¹¶å‘æ•°: 5, ä»»åŠ¡æ•°: {len(enriched_items)})...")
        tasks = []
        for item in enriched_items:
            tasks.append(self._process_single_item_async(item))
        
        await asyncio.gather(*tasks)
        logger.info("<<< æœ¬è½®æµå¼æ‰«æå®Œæˆã€‚")

    async def _process_single_item_async(self, raw_data):
        """å•æ¡æƒ…æŠ¥çš„å¼‚æ­¥å¤„ç†çŠ¶æ€æœº"""
        source_id = raw_data.get('source_id') or raw_data.get('id')
        
        async with self.ai_semaphore:
            try:
                # æ ‡è®°ä¸º PROCESSING é˜²æ­¢ç«äº‰
                await asyncio.to_thread(self.db.update_intelligence_status, source_id, 'PROCESSING')
                
                # 1. æ³¨å…¥ä¸Šä¸‹æ–‡ (åŒæ­¥æ–¹æ³•è½¬å¼‚æ­¥)
                context_str = await asyncio.to_thread(self._enrich_market_context, raw_data)
                
                # 2. è¯­ä¹‰å»é‡ (BERTçº§åˆ«)
                if self.deduplicator.is_duplicate(raw_data.get('content')):
                    logger.info(f"ğŸš« è¯­ä¹‰é‡å¤ï¼Œæ ‡è®°ä¸ºå·²è¿‡æ»¤: {source_id}")
                    await asyncio.to_thread(self.db.update_intelligence_status, source_id, 'COMPLETED', 'Deduplicated')
                    return

                # 3. AI åˆ†æ (å¼‚æ­¥)
                logger.info(f"ğŸ¤– æ­£åœ¨åˆ†æ({raw_data.get('extraction_method', 'UNKNOWN')}): {source_id}")
                try:
                    analysis_result = await self.primary_llm.analyze_async(raw_data)
                except Exception as e:
                    logger.warning(f"Primary LLM failed for {source_id}, trying fallback: {e}")
                    analysis_result = await self.fallback_llm.analyze_async(raw_data)

                # 4. å­˜å‚¨åˆ†æç»“æœå¹¶æ ‡è®°ä¸º COMPLETED
                if analysis_result:
                    analysis_result['embedding'] = self.deduplicator.last_vector
                    await asyncio.to_thread(self.db.update_intelligence_analysis, source_id, analysis_result, raw_data)

                    # 5. äº¤æ˜“é€»è¾‘ä¸åˆ†å‘
                    await self._trigger_trade_and_dispatch(analysis_result, raw_data)
                else:
                    raise ValueError("AI analysis returned empty result")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ¡ç›®å¤±è´¥ {source_id}: {e}")
                await asyncio.to_thread(self.db.update_intelligence_status, source_id, 'FAILED', str(e))

    async def _trigger_trade_and_dispatch(self, analysis_result, raw_data):
        """å¼‚æ­¥åŒ–çš„äº¤æ˜“è§¦å‘ä¸åˆ†å‘"""
        signal_direction = self._parse_sentiment(analysis_result.get('sentiment'))
        if signal_direction in ['Long', 'Short']:
            trade_initiated = await asyncio.to_thread(self.backtester.process_signal, signal_direction)
            if trade_initiated:
                logger.info(f"âœ… è§¦å‘äº¤æ˜“ä¿¡å·: {signal_direction} (ID: {raw_data.get('id')})")

        # å¤šæ¸ é“åˆ†å‘
        await asyncio.to_thread(self._dispatch, analysis_result)

    def run_once(self):
        """å…¼å®¹æ€§åŒ…è£…å™¨ï¼Œè°ƒç”¨å¼‚æ­¥æ–¹æ³•"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(self.run_once_async())
        finally:
            loop.close()

    def _enrich_market_context(self, raw_data):
        """æ³¨å…¥å¤šç»´åº¦å¸‚åœºèƒŒæ™¯æ•°æ® (DXY, GVZ, COT)"""
        now = datetime.now(pytz.utc)
        
        # 1. å®æ—¶è¡Œæƒ…å¿«ç…§
        dxy = self.db.get_market_snapshot("DX-Y.NYB", now)
        gvz = self.db.get_market_snapshot("^GVZ", now)
        
        # 2. æŒä»“æ•°æ® (Dimension B)
        cot = self.db.get_latest_indicator("COT_GOLD_NET", now)
        cot_info = "N/A"
        if cot:
            sentiment = "æ‹¥æŒ¤/è¶…ä¹°" if cot['percentile'] > 85 else "å†·æ·¡/è¶…å–" if cot['percentile'] < 15 else "ä¸­æ€§"
            cot_info = f"{cot['percentile']}% (çŠ¶æ€: {sentiment})"

        # 3. å®è§‚æ”¿ç­–èƒŒæ™¯ (Dimension D - Fed Backdrop)
        fed = self.db.get_latest_indicator("FED_REGIME", now)
        fed_context = "ä¸­æ€§ (Neutral)"
        if fed:
            fed_context = "é™æ¯å‘¨æœŸ/é¸½æ´¾ (Dovish)" if fed['value'] > 0 else "åŠ æ¯å‘¨æœŸ/é¹°æ´¾ (Hawkish)" if fed['value'] < 0 else "ä¸­æ€§ (Neutral)"

        context = f"""
[å½“å‰å¸‚åœºç¯å¢ƒå¿«ç…§]:
- ç¾å…ƒæŒ‡æ•° (DXY): {dxy if dxy else 'è·å–ä¸­'}
- é»„é‡‘æ³¢åŠ¨ç‡ (GVZ): {gvz if gvz else 'è·å–ä¸­'} (æŒ‡æ•° > 25 è¡¨ç¤ºææ…Œ/æµåŠ¨æ€§æ¯ç«­é£é™©)
- åŸºé‡‘æŒä»“æ‹¥æŒ¤åº¦ (COT): {cot_info}
- ç¾è”å‚¨å®è§‚åŸºè°ƒ (Regime): {fed_context}
"""
        raw_data['context'] = context
        raw_data['dxy_snapshot'] = dxy
        raw_data['gvz_snapshot'] = gvz
        raw_data['fed_val'] = fed['value'] if fed else 0
        
        return context

    def _dispatch(self, data):
        sentiment_text = ""
        sentiment = data.get('sentiment')
        if isinstance(sentiment, dict):
            sentiment_text = sentiment.get('zh') or sentiment.get('en') or "æƒ…æŠ¥åˆ†æ"
        else:
            sentiment_text = str(sentiment)

        title = f"ã€AlphaSignalã€‘{sentiment_text}"
        body = self._format_message(data)
        
        for channel in self.channels:
            try:
                channel.send(title, body)
            except Exception as e:
                logger.warning(f"Failed to dispatch to {channel.__class__.__name__}: {e}")

    def _parse_sentiment(self, sentiment_json) -> str:
        """æ ¹æ®æƒ…ç»ªæ–‡æœ¬ç¡®å®šäº¤æ˜“æ–¹å‘"""
        try:
            if isinstance(sentiment_json, str):
                try:
                    data = json.loads(sentiment_json.replace("'", '"'))
                except json.JSONDecodeError:
                    data = {'en': sentiment_json}
            else:
                data = sentiment_json
                
            text = str(data.get('en', '')).lower()
            
            if 'bullish' in text or 'safe-haven' in text or 'positive' in text or 'upward' in text:
                return 'Long'
            if 'bearish' in text or 'negative' in text or 'downward' in text or 'pressure' in text:
                return 'Short'
                
            return 'Neutral'
        except Exception as e:
            logger.warning(f"è§£ææƒ…ç»ªå¤±è´¥: {e}, é»˜è®¤ä¸ºä¸­æ€§ã€‚")
            return 'Neutral'

    def _format_message(self, data):
        market_impact_str = ""
        market_implication = data.get('market_implication', {})
        
        if isinstance(market_implication, dict):
            # Try to get localized version or iterate
            zh_impact = market_implication.get('zh')
            if zh_impact:
                market_impact_str = zh_impact
            else:
                for asset, impact in market_implication.items():
                    market_impact_str += f"ğŸ”¹ {asset}: {impact}\n"
        else:
            market_impact_str = str(market_implication)

        summary = data.get('summary', '')
        if isinstance(summary, dict): summary = summary.get('zh') or summary.get('en')

        advice = data.get('actionable_advice', '')
        if isinstance(advice, dict): advice = advice.get('zh') or advice.get('en')

        return f"""
ğŸš¨ ã€AlphaSignal æŠ•èµ„å¿«æŠ¥ã€‘
--------------------------------------------
ğŸ“Œ [æ ¸å¿ƒæ‘˜è¦]
{summary}

ğŸ“Š [å¸‚åœºæ·±åº¦å½±å“]
{market_impact_str.strip()}

ğŸ’¡ [å®æˆ˜ç­–ç•¥å»ºè®®]
{advice}

ğŸ”— [åŸæ–‡æ¥æºåŠé“¾æ¥]
{data.get('url')}
--------------------------------------------
(æ­¤æ¶ˆæ¯ç”± AlphaSignal AI å®æ—¶ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ)
"""
