import time
import json
from datetime import datetime
import pytz
from src.alphasignal.config import settings
from src.alphasignal.core.logger import logger

# å¼•å…¥ç»„ä»¶
from src.alphasignal.providers.data_sources.google_news import GoogleNewsSource
from src.alphasignal.providers.data_sources.rsshub import RSSHubSource
from src.alphasignal.providers.llm.gemini import GeminiLLM
from src.alphasignal.providers.llm.deepseek import DeepSeekLLM
from src.alphasignal.providers.channels.email import EmailChannel
from src.alphasignal.providers.channels.bark import BarkChannel
from src.alphasignal.core.database import IntelligenceDB
from src.alphasignal.core.backtest import BacktestEngine
from src.alphasignal.core.deduplication import NewsDeduplicator

class AlphaEngine:
    def __init__(self):
        self.sources = [
            GoogleNewsSource(),
            RSSHubSource()
        ]
        self.primary_llm = GeminiLLM()
        self.fallback_llm = DeepSeekLLM()
        self.channels = [EmailChannel(), BarkChannel()]
        self.db = IntelligenceDB()
        self.backtester = BacktestEngine(self.db)
        self.deduplicator = NewsDeduplicator()
        
        # Bootstrap deduplicator history from DB
        self._bootstrap_deduplicator()
        
    def _bootstrap_deduplicator(self):
        """Load recent intelligence from DB to initialize deduplicator history"""
        logger.info("ğŸ§µæ­£åœ¨åˆå§‹åŒ–å»é‡å¼•æ“å†å²æ•°æ®...")
        try:
            # Load intelligence from the last 24 hours
            recent_items = self.db.get_recent_intelligence(limit=200)
            if recent_items:
                # Process from oldest to newest to maintain correct history order
                for item in reversed(recent_items):
                    # Combine summary and content for consistent matching
                    summary = item.get('summary')
                    summary_text = ""
                    if isinstance(summary, dict):
                        summary_text = summary.get('en', '') or str(summary)
                    elif isinstance(summary, str):
                        summary_text = summary
                    
                    text = summary_text if len(summary_text) > 20 else (item.get('content') or "")
                    if text:
                        # We don't use is_duplicate here to avoid redundant checks, 
                        # just populate the internal history
                        clean_text = self.deduplicator.normalize(text)
                        if clean_text:
                            from simhash import Simhash
                            sh = Simhash(clean_text)
                            vec = None
                            if self.deduplicator.model:
                                try:
                                    vec = self.deduplicator.model.encode(clean_text)
                                except Exception as e:
                                    logger.warning(f"Semantic encoding failed during bootstrap: {e}")
                            
                            self.deduplicator.add_to_history(sh, vec, record_id=item.get('id'))
                
                logger.info(f"âœ… å·²åŠ è½½ {len(self.deduplicator.simhash_history)} æ¡è®°å½•åˆ°å»é‡å¼•æ“å†å²ã€‚")
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å»é‡å¼•æ“å¤±è´¥: {e}")

        
    def run_once(self):
        logger.info(">>> å¼€å§‹ä¸€è½®æ–°çš„æƒ…æŠ¥æ‰«æ...")
        
        # 0. æ•°æ®å›å¡«
        self.backtester.sync_outcomes()

        # 1. è·å–æ‰€æœ‰æ•°æ®æºçš„æ–°æƒ…æŠ¥
        new_items = []
        for source in self.sources:
            try:
                items = source.fetch()
                if items:
                    if isinstance(items, list):
                        new_items.extend(items)
                    else:
                        new_items.append(items)
            except Exception as e:
                logger.error(f"æ•°æ®æºæ‰«æå¼‚å¸¸: {e}")

        if not new_items:
            logger.info("æ— æ–°æƒ…æŠ¥ï¼Œæœ¬è½®ç»“æŸã€‚")
            return

        logger.info(f"æœ¬è½®å…±å‘ç° {len(new_items)} æ¡æ–°æƒ…æŠ¥ï¼Œå¼€å§‹é€ä¸€å¤„ç†...")

        # 2. é€æ¡å¤„ç†
        for raw_data in new_items:
            self._process_single_item(raw_data)
            
        logger.info("<<< æœ¬è½®æ‰«æå®Œæˆã€‚")

    def _process_single_item(self, raw_data):
        """å¤„ç†å•æ¡æƒ…æŠ¥çš„æ ¸å¿ƒæµç¨‹"""
        # 0. å»é‡æ£€æŸ¥ (æ–°å¢)
        news_url = raw_data.get('url')
        news_content = raw_data.get('content')
        news_summary = raw_data.get('summary')
        
        # 1.5 è·å–å¸‚åœºä¸Šä¸‹æ–‡ä¸å†å²ç½®ä¿¡åº¦ (Dimension A-C)
        context_str = self._enrich_market_context(raw_data)
        
        # ç®€å•æå–å…³é”®è¯ä½œä¸ºä¸Šä¸‹æ–‡å›æµ‹æœç´¢
        keyword = "Trump" 
        if "Fed" in raw_data.get('content', ''): keyword = "Fed"
        
        stats = self.backtester.get_confidence_stats(keyword)
        if stats:
            bt_str = f"\n[å†å²å›æµ‹é¢æ¿]: è¿‡å» {stats['count']} æ¬¡ç›¸å…³äº‹ä»¶ä¸­ (å…³é”®è¯:{keyword})ï¼Œé»„é‡‘ä¸Šæ¶¨æ¦‚ç‡ {stats['win_rate']}%, å¹³å‡æ³¢å¹… {stats['avg_return']}%." 
            raw_data['content'] += bt_str
            context_str += bt_str

        # 1. è¯­ä¹‰å»é‡ (SimHash + BERT)
        # æå‰æ£€æŸ¥ï¼Œè‹¥è¯­ä¹‰é‡å¤ï¼Œç›´æ¥ä¸¢å¼ƒä¸”ä¸å…¥åº“
        full_text = news_summary if (news_summary and len(str(news_summary)) > 20) else news_content
        
        # æ­¤æ—¶è¿˜æ²¡æœ‰å…¥åº“ IDï¼Œrecord_id ä¼  None (ä»…åšå†…å­˜è®°å½•)
        if self.deduplicator.is_duplicate(full_text, record_id=None):
            logger.info(f"ğŸš« å‘ç°è¯­ä¹‰é‡å¤æƒ…æŠ¥ (BERTçº§åˆ«)ï¼Œç›´æ¥ä¸¢å¼ƒ: {raw_data.get('title') or news_content[:50]}...")
            return

        # 2. ä¿å­˜åŸå§‹æƒ…æŠ¥å…¥åº“ (Save Raw)
        # è¿™å°†è¿”å› IDï¼Œå¦‚æœå›  URL/SourceID å†²çªè¿”å› Noneï¼Œåˆ™ä¸ºé‡å¤
        # æ³¨æ„: save_raw_intelligence ä½¿ç”¨ raw_data['content']
        db_id = self.db.save_raw_intelligence(raw_data)
        if not db_id:
            logger.info(f"ğŸš« å‘ç°é‡å¤æƒ…æŠ¥ (SourceIDå†²çª)ï¼Œå·²å­˜åœ¨ï¼Œè·³è¿‡: URL: {news_url}")
            return

        # 3. AI åˆ†æ
        analysis_result = None
        try:
            logger.info(f"æ­£åœ¨åˆ†ææƒ…æŠ¥: {raw_data.get('source')} - {raw_data.get('id')}")
            analysis_result = self.primary_llm.analyze(raw_data)
        except Exception:
            logger.warning("é¦–é€‰æ¨¡å‹å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹...")
            try:
                analysis_result = self.fallback_llm.analyze(raw_data)
            except Exception:
                logger.error(f"AI åˆ†æå¤±è´¥ï¼Œè·³è¿‡: {raw_data.get('id')}")
                return

        if not analysis_result:
            return

        # 4. ç»“æœæ›´æ–°ä¸å­˜å‚¨ (Update Analysis)
        # å»æ‰æ³¨å…¥çš„ä¸Šä¸‹æ–‡ï¼Œä¿æŒçº¯å‡€ (raw_dataå·²åœ¨ save_raw æ—¶ä½¿ç”¨äº† dirty content? 
        # save_raw ä½¿ç”¨äº† raw_data['content']ã€‚
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦ clean content å—? 
        # engine.py 150 original logic: saved clean_content.
        # save_raw saved raw_data['content'] which includes bt_str context.
        # Ideally we should strictly save original content.
        # But 'raw_data' passed to save_raw had bt_str appended.
        # To fix this, we should clean raw_data['content'] AFTER analysis, or before save_raw passing a copy?
        # Simpler: remove context_str from raw_data['content'] before save_raw?
        # But we need context for AI analysis.
        # It's fine if Raw Data in DB has context string appended, it shows what AI saw.
        # User might prefer clean. 
        # Let's clean it for 'original_content' field usage in save_raw if possible.
        # raw_data['original_content'] is usually not set yet.
        
        clean_content = raw_data.get('content').replace(context_str, "")
        analysis_result['original_content'] = clean_content
        analysis_result['url'] = raw_data.get('url')
        
        # Update existing record
        self.db.update_intelligence_analysis(raw_data.get('id'), analysis_result, raw_data)

        # Apply Intraday Directional Deduplication
        signal_direction = self._parse_sentiment(analysis_result.get('sentiment'))
        if signal_direction in ['Long', 'Short']:
            trade_initiated = self.backtester.process_signal(signal_direction)
            if trade_initiated:
                logger.info(f"âœ… è§¦å‘äº¤æ˜“ä¿¡å·: {signal_direction} (æ¥è‡ª intelligence ID: {raw_data.get('id')})")
            else:
                logger.info(f"â„¹ï¸ æœªè§¦å‘äº¤æ˜“: ä¿¡å· {signal_direction} è¢«æ—¥å†…åŒå‘å»é‡è·³è¿‡ (æ¥è‡ª intelligence ID: {raw_data.get('id')})")

        # 4. å¤šæ¸ é“åˆ†å‘
        self._dispatch(analysis_result)

    def _enrich_market_context(self, raw_data):
        """æ³¨å…¥å¤šç»´åº¦å¸‚åœºèƒŒæ™¯æ•°æ® (DXY, GVZ, COT)"""
        now = datetime.now(pytz.utc)
        
        # 1. å®æ—¶è¡Œæƒ…å¿«ç…§
        dxy = self.db.get_market_snapshot("DX-Y.NYB", now)
        gvz = self.db.get_market_snapshot("^GVZ", now)
        
        # 2. æŒä»“æ•°æ® (Dimension B)
        cot = self.db.get_latest_indicator("COT_GOLD_NET", now)
        cot_info = "N/A"
        if cot:
            sentiment = "æ‹¥æŒ¤/è¶…ä¹°" if cot['percentile'] > 85 else "å†·æ·¡/è¶…å–" if cot['percentile'] < 15 else "ä¸­æ€§"
            cot_info = f"{cot['percentile']}% (çŠ¶æ€: {sentiment})"

        # 3. å®è§‚æ”¿ç­–èƒŒæ™¯ (Dimension D - Fed Backdrop)
        fed = self.db.get_latest_indicator("FED_REGIME", now)
        fed_context = "ä¸­æ€§ (Neutral)"
        if fed:
            fed_context = "é™æ¯å‘¨æœŸ/é¸½æ´¾ (Dovish)" if fed['value'] > 0 else "åŠ æ¯å‘¨æœŸ/é¹°æ´¾ (Hawkish)" if fed['value'] < 0 else "ä¸­æ€§ (Neutral)"

        context = f"""
[å½“å‰å¸‚åœºç¯å¢ƒå¿«ç…§]:
- ç¾å…ƒæŒ‡æ•° (DXY): {dxy if dxy else 'è·å–ä¸­'}
- é»„é‡‘æ³¢åŠ¨ç‡ (GVZ): {gvz if gvz else 'è·å–ä¸­'} (æŒ‡æ•° > 25 è¡¨ç¤ºææ…Œ/æµåŠ¨æ€§æ¯ç«­é£é™©)
- åŸºé‡‘æŒä»“æ‹¥æŒ¤åº¦ (COT): {cot_info}
- ç¾è”å‚¨å®è§‚åŸºè°ƒ (Regime): {fed_context}
"""
        raw_data['context'] = context
        # åŒæ—¶ä¹Ÿä¸´æ—¶å­˜å…¥ raw_data æ–¹ä¾¿æ•°æ®åº“ä¿å­˜æ—¶å¤ç”¨
        raw_data['dxy_snapshot'] = dxy
        raw_data['gvz_snapshot'] = gvz
        raw_data['fed_val'] = fed['value'] if fed else 0
        
        return context

    def _dispatch(self, data):
        title = f"ã€AlphaSignalã€‘{data.get('sentiment', 'æƒ…æŠ¥è­¦æŠ¥')}"
        body = self._format_message(data)
        
        for channel in self.channels:
            channel.send(title, body)

    def _parse_sentiment(self, sentiment_json) -> str:
        """
        æ ¹æ®æƒ…ç»ªæ–‡æœ¬ç¡®å®šäº¤æ˜“æ–¹å‘ã€‚
        Returns: 'Long', 'Short', or 'Neutral'
        """
        try:
            if isinstance(sentiment_json, str):
                try:
                    data = json.loads(sentiment_json.replace("'", '"'))
                except json.JSONDecodeError:
                    data = {'en': sentiment_json}
            else:
                data = sentiment_json
                
            text = str(data.get('en', '')).lower()
            
            if 'bullish' in text or 'safe-haven' in text or 'positive' in text or 'upward' in text:
                return 'Long'
            if 'bearish' in text or 'negative' in text or 'downward' in text or 'pressure' in text:
                return 'Short'
                
            return 'Neutral'
        except Exception as e:
            logger.warning(f"è§£ææƒ…ç»ªå¤±è´¥: {e}, é»˜è®¤ä¸ºä¸­æ€§ã€‚")
            return 'Neutral'

    def _format_message(self, data):
        # æ ¼å¼åŒ–å¸‚åœºå½±å“éƒ¨åˆ†
        market_impact_str = ""
        market_implication = data.get('market_implication', {})
        
        if isinstance(market_implication, dict):
            for asset, impact in market_implication.items():
                market_impact_str += f"ğŸ”¹ {asset}: {impact}\n"
        else:
            market_impact_str = str(market_implication)

        return f"""
ğŸš¨ ã€AlphaSignal æŠ•èµ„å¿«æŠ¥ã€‘
--------------------------------------------
ğŸ“Œ [æ ¸å¿ƒæ‘˜è¦]
{data.get('summary')}

ğŸ“Š [å¸‚åœºæ·±åº¦å½±å“]
{market_impact_str.strip()}

ğŸ’¡ [å®æˆ˜ç­–ç•¥å»ºè®®]
{data.get('actionable_advice')}

ğŸ”— [åŸæ–‡æ¥æºåŠé“¾æ¥]
{data.get('url')}
--------------------------------------------
(æ­¤æ¶ˆæ¯ç”± AlphaSignal AI å®æ—¶ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ)
"""