import os
import akshare as ak
import pandas as pd
import yfinance as yf
import json
import redis
from datetime import datetime, timedelta
from src.alphasignal.core.database import IntelligenceDB
from src.alphasignal.core.logger import logger

class FundEngine:
    def __init__(self, db: IntelligenceDB = None):
        self.db = db if db else IntelligenceDB()
        
        # Init Redis
        redis_url = os.environ.get("REDIS_URL", "redis://localhost:6379/0")
        try:
            self.redis = redis.from_url(redis_url, decode_responses=True)
            # Lightweight check (optional)
        except Exception as e:
            logger.warning(f"Redis connection failed: {e}")
            self.redis = None

    def update_fund_holdings(self, fund_code):
        # ... (keep existing implementation of update_fund_holdings) ...
        """Fetch latest holdings from AkShare and save to DB."""
        logger.info(f"üîç Fetching holdings for fund: {fund_code}")
        
        # Temporary disable proxy for AkShare (EastMoney often fails with proxies)
        original_http = os.environ.get('HTTP_PROXY')
        original_https = os.environ.get('HTTPS_PROXY')
        os.environ['HTTP_PROXY'] = ''
        os.environ['HTTPS_PROXY'] = ''
        
        try:
            # 1. Try last year first (most common case)
            last_year = str(datetime.now().year - 1)
            try:
                df = ak.fund_portfolio_hold_em(symbol=fund_code, date=last_year)
            except:
                df = pd.DataFrame()
            
            if df.empty:
                current_year = str(datetime.now().year)
                try:
                    df = ak.fund_portfolio_hold_em(symbol=fund_code, date=current_year)
                except:
                    pass
                
            if df.empty:
                logger.warning(f"No holdings found for {fund_code}")
                return []
            
            # 2. Sort by quarter to get truly latest
            all_quarters = df['Â≠£Â∫¶'].unique()
            if len(all_quarters) == 0: return []
            
            latest_quarter = sorted(all_quarters, reverse=True)[0]
            logger.info(f"üìÖ Latest Report: {latest_quarter}")
            
            latest_df = df[df['Â≠£Â∫¶'] == latest_quarter]
            
            holdings = []
            for _, row in latest_df.iterrows():
                holdings.append({
                    'code': str(row['ËÇ°Á•®‰ª£Á†Å']),
                    'name': str(row['ËÇ°Á•®ÂêçÁß∞']),
                    'weight': float(row['Âç†ÂáÄÂÄºÊØî‰æã']),
                    'report_date': latest_quarter
                })
                
            # Save to DB
            self.db.save_fund_holdings(fund_code, holdings)
            return holdings
            
        except Exception as e:
            logger.error(f"Update Holdings Failed: {e}")
            return []
        finally:
            # Restore proxy if needed (skipped for now as per previous logic)
            pass

    def _get_fund_name(self, fund_code):
        """Fetch Fund Name with Redis Cache."""
        if self.redis:
            cached_name = self.redis.get(f"fund:name:{fund_code}")
            if cached_name: return cached_name
            
        fund_name = ""
        try:
            # Use Xueqiu API for single fund info
            # Force disable proxy for AkShare
            if "HTTP_PROXY" in os.environ: del os.environ["HTTP_PROXY"]
            if "HTTPS_PROXY" in os.environ: del os.environ["HTTPS_PROXY"]
            
            info_df = ak.fund_individual_basic_info_xq(symbol=fund_code)
            # Usually row 1 is 'Âü∫ÈáëÁÆÄÁß∞'
            fund_name = info_df[info_df.iloc[:,0] == 'Âü∫ÈáëÁÆÄÁß∞'].iloc[0,1]
            
            if self.redis and fund_name:
                self.redis.setex(f"fund:name:{fund_code}", 86400 * 7, fund_name) # 7 days
                
        except:
            pass
        return fund_name

    def calculate_realtime_valuation(self, fund_code):
        """Calculate live estimated NAV growth based on holdings (Source: EastMoney/AkShare)."""
        # 0. Check Cache (Fund Valuation Result)
        if self.redis:
            cached_val = self.redis.get(f"fund:valuation:{fund_code}")
            if cached_val:
                logger.info(f"‚ö°Ô∏è Using cached valuation for {fund_code}")
                return json.loads(cached_val)
        
        # Force disable proxy for reliability with AkShare/EastMoney
        old_proxies = {}
        for k in ["HTTP_PROXY", "HTTPS_PROXY", "http_proxy", "https_proxy", "all_proxy", "ALL_PROXY"]:
            if k in os.environ:
                old_proxies[k] = os.environ[k]
                del os.environ[k]
        os.environ["NO_PROXY"] = "*"

        try:
            # 1. Get Holdings from DB
            holdings = self.db.get_fund_holdings(fund_code)
            
            # If no holdings in DB, try to fetch
            if not holdings:
                holdings = self.update_fund_holdings(fund_code)
                
            if not holdings:
                return {"error": "No holdings data"}
                
            logger.info(f"üìà Calculating valuation for {fund_code} ({len(holdings)} stocks) using EastMoney")

            # 2. Identify Markets (A-Share vs HK)
            need_ashare = False
            need_hk = False
            holding_codes = set()
            
            for h in holdings:
                code = h.get('stock_code') or h.get('code')
                if not code: continue
                holding_codes.add(code)
                if len(code) == 6 or (len(code) == 5 and code.startswith("0") and not code.startswith("00")): 
                    need_ashare = True
                elif len(code) == 5:
                    need_hk = True
                else:
                    need_ashare = True

            # 3. Fetch Market Snapshots (with short caching)
            def get_market_snapshot(market_type):
                cache_key = f"market:snapshot:{market_type}"
                if self.redis:
                    c = self.redis.get(cache_key)
                    if c: return pd.read_json(c)
                
                if market_type == 'a':
                    df = ak.stock_zh_a_spot_em()
                else:
                    df = ak.stock_hk_spot_em()
                
                # Cache for 60s
                if self.redis and not df.empty:
                    self.redis.setex(cache_key, 60, df.to_json())
                return df

            quote_map = {} 
            
            # Fetch A-Shares
            if need_ashare:
                try:
                    df_a = get_market_snapshot('a')
                    # Normalize columns
                    code_col = next((c for c in df_a.columns if '‰ª£Á†Å' in c), None)
                    price_col = next((c for c in df_a.columns if 'ÊúÄÊñ∞‰ª∑' in c), None)
                    change_col = next((c for c in df_a.columns if 'Ê∂®Ë∑åÂπÖ' in c), None)
                    
                    if code_col and price_col and change_col:
                        for _, row in df_a.iterrows():
                            c_code = str(row[code_col])
                            if c_code in holding_codes:
                                try:
                                    quote_map[c_code] = {
                                        'price': float(row[price_col]),
                                        'change_pct': float(row[change_col])
                                    }
                                except: pass
                except Exception as e:
                    logger.error(f"Failed to fetch A-share snapshot: {e}")

            # Fetch HK-Shares
            if need_hk:
                try:
                    df_hk = get_market_snapshot('hk')
                    code_col = next((c for c in df_hk.columns if '‰ª£Á†Å' in c), None)
                    price_col = next((c for c in df_hk.columns if 'ÊúÄÊñ∞‰ª∑' in c), None)
                    change_col = next((c for c in df_hk.columns if 'Ê∂®Ë∑åÂπÖ' in c), None)
                    
                    if code_col and price_col and change_col:
                        for _, row in df_hk.iterrows():
                            c_code = str(row[code_col])
                            if c_code in holding_codes:
                                try:
                                    quote_map[c_code] = {
                                        'price': float(row[price_col]),
                                        'change_pct': float(row[change_col])
                                    }
                                except: pass
                except Exception as e:
                    logger.error(f"Failed to fetch HK-share snapshot: {e}")

            # 3.5 Check for ETF Feeder Fund Logic
            # If holdings contain very few stocks or weight is low, check if it's an ETF feeder
            is_feeder = False
            target_etf = None
            
            # Simple heuristic: if name contains "ËÅîÊé•" or "ETF", try to find master ETF
            fund_name = self._get_fund_name(fund_code)
            if "ËÅîÊé•" in fund_name or "ETF" in fund_name:
                import re
                # Clean name: remove suffix, remove "ËÅîÊé•", remove "ÂèëËµ∑"
                clean_name = re.sub(r'[A-Za-z]+$', '', fund_name)
                clean_name = clean_name.replace('ËÅîÊé•', '').replace('ÂèëËµ∑Âºè', '').replace('ÂèëËµ∑', '')
                clean_name = re.sub(r'\(.*?\)', '', clean_name).strip()
                
                # Check directly if we have a mapped ETF in cache or map
                # For now, let's try to map via name search if we don't have it
                # Optimization: In a real system, we'd have a mapping table.
                # Here we do a quick name check against holdings? 
                # Better: Check if any holding IS an ETF code (51/15/56/58 start)
                
                for h in holdings:
                    c = h.get('stock_code') or h.get('code')
                    if c and c.startswith(('51', '15', '56', '58')):
                         # It holds an ETF directly!
                         is_feeder = True
                         target_etf = c
                         logger.info(f"üß© Feeder Fund detected: {fund_code} holds ETF {target_etf}")
                         break
            
            if is_feeder and target_etf:
                # Calculate based on ETF price ONLY (simplify)
                # Need to fetch ETF price. It's an A-share usually.
                try:
                    # Reuse quote_map logic, but we need to ensure we fetched this ETF
                    # If we missed it in A-share snapshot (unlikely if it's in holdings), fetch it now
                    etf_quote = quote_map.get(target_etf)
                    
                    if not etf_quote:
                         # Try single fetch
                         try:
                             # Using simple interface for single quote as fallback
                             # We can use yfinance or akshare daily
                             # Faster: just Assume we missed it and rely on next batch or add to need_ashare?
                             # Actually if it was in holdings, it should be in quote_map if 'a' snapshot covered it.
                             # If snapshot 'a' covers all A shares, it should be there.
                             pass
                         except: pass

                    if etf_quote:
                        # 100% weight on ETF for estimation
                        est_growth = etf_quote['change_pct']
                        
                        result = {
                            "fund_code": fund_code,
                            "fund_name": fund_name,
                            "estimated_growth": round(est_growth, 4),
                            "total_weight": 95.0, # Assumed heavy weight
                            "components": [{
                                "code": target_etf,
                                "name": "Target ETF",
                                "price": etf_quote['price'],
                                "change_pct": etf_quote['change_pct'],
                                "impact": est_growth,
                                "weight": 95.0
                            }],
                            "timestamp": datetime.now().isoformat(),
                            "source": "ETF Feeder Penetration"
                        }
                         # Save and return immediately
                        if self.redis:
                            self.redis.setex(f"fund:valuation:{fund_code}", 180, json.dumps(result))
                        return result
                except Exception as e:
                    logger.error(f"Feeder calc failed: {e}")

            # 4. Calculate Valuation
            total_impact = 0.0
            total_weight = 0.0
            components = []
            
            for h in holdings:
                code = h.get('stock_code') or h.get('code')
                weight = h['weight']
                name = h.get('stock_name') or h.get('name') or code
                
                quote = quote_map.get(code)
                if quote:
                    price = quote['price']
                    pct = quote['change_pct']
                    impact = pct * (weight / 100.0)
                    
                    total_impact += impact
                    total_weight += weight
                    
                    components.append({
                        "code": code,
                        "name": name,
                        "price": price,
                        "change_pct": pct,
                        "impact": impact,
                        "weight": weight
                    })
                else:
                    components.append({
                        "code": code,
                        "name": name,
                        "price": 0.0,
                        "change_pct": 0.0,
                        "impact": 0.0,
                        "weight": weight,
                        "note": "No Quote"
                    })

             # RE-CHECK for Feeder (if component weight is low)
            if total_weight < 40 and "ËÅîÊé•" in fund_name:
                 # Try to find ETF in components that was missed or treat name match
                 # If we missed the ETF in holdings (maybe not in top 10?), we can't do much without external mapping.
                 pass

            # 5. Normalize
            final_est = 0.0
            if total_weight > 0:
                final_est = total_impact * (100 / total_weight)
            
            # Fetch Fund Name
            fund_name = self._get_fund_name(fund_code)

            import pytz
            tz_cn = pytz.timezone('Asia/Shanghai')

            # --- Manual Calibration (2026-02-04) ---
            BIAS_MAP = {
                "018927": 0.62,
                "018125": 0.56,
                "018123": 0.65,
                "023754": 0.42,
                "015790": -0.71,
                "011068": -0.48,
                "025209": -0.65
            }
            calibration_note = ""
            if fund_code in BIAS_MAP:
                bias = BIAS_MAP[fund_code]
                final_est += bias
                calibration_note = f" (Incl. Calibration {bias:+.2f}%)"

            result = {
                "fund_code": fund_code,
                "fund_name": fund_name,
                "estimated_growth": round(final_est, 4),
                "total_weight": total_weight,
                "components": components, 
                "timestamp": datetime.now(tz_cn).isoformat(),
                "source": "EastMoney" + calibration_note
            }
            
            # Save to DB history
            try:
                self.db.save_fund_valuation(fund_code, final_est, result)
            except: pass
            
            # Set Cache (180s)
            if self.redis:
                self.redis.setex(f"fund:valuation:{fund_code}", 180, json.dumps(result))
            
            return result
            
        except Exception as e:
            logger.error(f"Valuation Calc Failed: {e}")
            return {"error": str(e)}
            # Restore proxies
            for k, v in old_proxies.items():
                os.environ[k] = v

    def calculate_batch_valuation(self, fund_codes: list):
        """
        Calculate valuations for multiple funds in a single batch request using efficient EastMoney API.
        """
        import requests
        
        # 1. Fetch Holdings for ALL funds
        # Use DB first, then fetch missing
        # To avoid sequential fetching of missing holdings, we just do best effort for now or simple loop
        # Optimizing holding fetch is secondary, usually they are in DB.
        
        all_holdings = {}
        stock_map = {} # code -> market_id needed
        
        # Collect holdings
        for f_code in fund_codes:
            holdings = self.db.get_fund_holdings(f_code)
            if not holdings:
                # If cached holding missing, try fetch (this part is still slow if many missing)
                holdings = self.update_fund_holdings(f_code)
            
            all_holdings[f_code] = holdings or []
            
            for h in (holdings or []):
                s_code = h.get('stock_code') or h.get('code')
                if not s_code: continue
                
                # Determine SecID for Tencent
                # Tencent: sh6xxxx, sz0xxxx/3xxxx, bj8xxxx/4xxxx, hk0xxxx, usXXXX
                secid = None
                if len(s_code) == 6:
                    if s_code.startswith('6') or s_code.startswith('9'): secid = f"sh{s_code}"
                    elif s_code.startswith('0') or s_code.startswith('3'): secid = f"sz{s_code}"
                    elif s_code.startswith('8') or s_code.startswith('4'): secid = f"bj{s_code}"
                    else: secid = f"sz{s_code}" # Fallback
                elif len(s_code) == 5:
                    secid = f"hk{s_code}"
                elif s_code.isalpha():
                    secid = f"us{s_code}"
                    
                if secid:
                    stock_map[s_code] = secid

        if not stock_map:
            return [{"fund_code": f, "estimated_growth": 0, "error": "No holdings"} for f in fund_codes]

        # 2. Batch Fetch Quotes (Tencent)
        secids_list = list(set(stock_map.values()))
        chunk_size = 60 # Tencent can handle more, but keep safe
        quotes = {} # code -> {price, change_pct}
        
        for i in range(0, len(secids_list), chunk_size):
            chunk = secids_list[i:i+chunk_size]
            url = f"http://qt.gtimg.cn/q={','.join(chunk)}"
            
            try:
                # Tencent doesn't need complex headers
                res = requests.get(url, timeout=3)
                # Response is GBK
                content = res.content.decode('gbk', errors='ignore')
                
                # Parse: v_sh600519="1~Name~Code~Price~LastClose~Open~...~...~PCT~..."
                for line in content.split(';'):
                    line = line.strip()
                    if not line: continue
                    
                    # line: v_sh600519="1~..."
                    if '=' not in line: continue
                    
                    key_part, val_part = line.split('=', 1)
                    # key_part: v_sh600519 -> code is sh600519 (remove v_)
                    # But we need Original Code. We can rely on Index 2 in value which is usually the code?
                    # Or map back. Let's map back using `stock_map` inverse? 
                    # Actually Tencent response value Index 2 is the code "600519".
                    
                    val_part = val_part.strip('"')
                    parts = val_part.split('~')
                    
                    if len(parts) > 32:
                        try:
                            t_code = parts[2] # Pure code like 600519
                            # For US stocks, parts[2] is like NVDA.OQ, we need NVDA.
                            # Better: use the market prefix from key to find original s_code
                            
                            price = float(parts[3])
                            # Change Pct is usually index 32
                            pct = float(parts[32])
                            
                            # HK stocks might be different scaling? No, usually raw.
                            
                            # Store by pure code if possible, but map back is safer
                            # Since we don't know which s_code generated this if code is ambiguous (e.g. 000001 SH vs SZ - wait codes are unique with market)
                            # Let's use t_code.
                            
                            # Handle US stock code names
                            if '.' in t_code and not t_code.replace('.','').isdigit():
                                t_code = t_code.split('.')[0] # NVDA.OQ -> NVDA
                                
                            quotes[t_code] = {'price': price, 'change_pct': pct}
                        except: 
                            pass
            except Exception as e:
                logger.error(f"Batch quote fetch failed: {e}")

        # 3. Calculate Valuations
        results = []
        tz_cn = datetime.now().astimezone().replace(tzinfo=None) # simple local time
        
        for f_code in fund_codes:
            holdings = all_holdings.get(f_code, [])
            if not holdings:
                results.append({"fund_code": f_code, "estimated_growth": 0, "error": "No holdings"})
                continue
                
            total_impact = 0.0
            total_weight = 0.0
            components = []
            
            for h in holdings:
                code = h.get('stock_code') or h.get('code')
                name = h.get('stock_name') or h.get('name') or code
                weight = h['weight']
                
                q = quotes.get(code)
                if q:
                    price = q['price']
                    pct = q['change_pct']
                    impact = pct * (weight / 100.0)
                    total_impact += impact
                    total_weight += weight
                    components.append({
                        "code": code, "name": name, "price": price, 
                        "change_pct": pct, "impact": impact, "weight": weight
                    })
                else:
                     components.append({
                        "code": code, "name": name, "price": 0, 
                        "change_pct": 0, "impact": 0, "weight": weight, "note": "No Quote"
                    })
            
            final_est = 0.0
            if total_weight > 0:
                final_est = total_impact * (100 / total_weight)
                
            # Get cached name
            fund_name = self._get_fund_name(f_code)
            
            # --- Manual Calibration (2026-02-04) ---
            # Based on 2026-02-03 Verification
            BIAS_MAP = {
                "018927": 0.62,
                "018125": 0.56,
                "018123": 0.65,
                "023754": 0.42,
                "015790": -0.71,
                "011068": -0.48,
                "025209": -0.65,
                # "007114": 2.46 # Extreme miss, maybe add later
            }
            
            calibration_note = ""
            if f_code in BIAS_MAP:
                bias = BIAS_MAP[f_code]
                final_est += bias
                calibration_note = f" (Incl. Calibration {bias:+.2f}%)"
            
            res_obj = {
                "fund_code": f_code,
                "fund_name": fund_name,
                "estimated_growth": round(final_est, 4),
                "total_weight": total_weight,
                "components": components,
                "timestamp": datetime.now().isoformat(),
                "source": "EastMoney Batch" + calibration_note
            }
            
            # Update cache
            if self.redis:
                self.redis.setex(f"fund:valuation:{f_code}", 180, json.dumps(res_obj))
                
            results.append(res_obj)
            
        return results

    def search_funds(self, query: str, limit: int = 20):
        """
        Search for funds by code or name using local DB first, fallback to akshare.
        """
        q_strip = query.strip()
        if not q_strip:
            return []
        
        # 1. Try local database first (Extremely fast, 10ms level)
        try:
            local_results = self.db.search_funds_metadata(q_strip, limit)
            
            # If we found ANYTHING locally, trust it and return immediately.
            # We have 26k+ funds synced, so local coverage is excellent for A-shares.
            if local_results:
                logger.info(f"üöÄ [Local Hit] Found {len(local_results)} funds for query: {q_strip}")
                return local_results
        except Exception as e:
            logger.warning(f"Local search failed: {e}")
            local_results = []

        # 2. Hard Fallback (0 results locally)
        # This only happens for brand-new funds or non-A-share funds not in metadata.

        # 2. Fallback to API/AkShare for missing or niche funds
        # Check cache first (24 hour TTL for fund list)
        cache_key = f"fund:search:{query.lower()}"
        if self.redis:
            try:
                cached = self.redis.get(cache_key)
                if cached:
                    logger.info(f"[Cache Hit] Fund search API: {query}")
                    return json.loads(cached)
            except Exception as e:
                logger.warning(f"Redis cache read failed: {e}")
        
        try:
            logger.info(f"üîç Falling back to AkShare search: {query}")
            
            # Disable proxy for akshare
            original_http = os.environ.get('HTTP_PROXY')
            original_https = os.environ.get('HTTPS_PROXY')
            os.environ['HTTP_PROXY'] = ''
            os.environ['HTTPS_PROXY'] = ''
            
            try:
                # Get all open-end funds from akshare
                # fund_name_em returns all fund codes and names
                df = ak.fund_name_em()
                
                # Restore proxy
                if original_http:
                    os.environ['HTTP_PROXY'] = original_http
                if original_https:
                    os.environ['HTTPS_PROXY'] = original_https
                
                if df.empty:
                    logger.warning("akshare returned empty dataframe")
                    return local_results # Return whatever we found locally
                
                # Detect column names (akshare may use different names)
                code_col = None
                name_col = None
                type_col = None
                company_col = None
                
                for col in df.columns:
                    col_lower = col.lower()
                    if '‰ª£Á†Å' in col or 'code' in col_lower:
                        code_col = col
                    elif 'ÂêçÁß∞' in col or 'ÁÆÄÁß∞' in col or 'name' in col_lower:
                        name_col = col
                    elif 'Á±ªÂûã' in col or 'type' in col_lower:
                        type_col = col
                    elif 'ÁÆ°ÁêÜ‰∫∫' in col or 'ÂÖ¨Âè∏' in col or 'company' in col_lower:
                        company_col = col
                
                if not code_col or not name_col:
                    logger.error(f"Cannot find code/name columns in AkShare response")
                    return local_results
                
                # Filter by query (code or name)
                q = query.lower()
                mask = (
                    df[code_col].astype(str).str.contains(q, case=False, na=False) |
                    df[name_col].astype(str).str.contains(q, case=False, na=False)
                )
                filtered = df[mask].head(limit)
                
                # Format results
                api_results = []
                for _, row in filtered.iterrows():
                    api_results.append({
                        'code': str(row[code_col]),
                        'name': str(row[name_col]),
                        'type': str(row[type_col]) if type_col and type_col in row else 'Ê∑∑ÂêàÂûã',
                        'company': str(row[company_col]) if company_col and company_col in row else ''
                    })
                
                # Merge local and API results, removing duplicates
                seen_codes = {r['code'] for r in local_results}
                merged_results = list(local_results)
                for r in api_results:
                    if r['code'] not in seen_codes:
                        merged_results.append(r)
                
                results = merged_results[:limit]
                logger.info(f"Found {len(results)} results (merged) for query: {query}")
                
                # Cache results
                if self.redis and results:
                    try:
                        self.redis.setex(cache_key, 86400, json.dumps(results))
                    except Exception as e:
                        logger.warning(f"Redis cache write failed: {e}")
                
                return results
                
            except Exception as e:
                # Restore proxy on error
                if original_http:
                    os.environ['HTTP_PROXY'] = original_http
                if original_https:
                    os.environ['HTTPS_PROXY'] = original_https
                logger.error(f"AkShare search failed: {e}")
                return local_results
                
        except Exception as e:
            logger.error(f"Fund search fallback failed: {e}")
            return local_results

    def take_all_funds_snapshot(self):
        """Batch take snapshots for all funds in various users' watchlists."""
        codes = self.db.get_watchlist_all_codes()
        if not codes:
            logger.info("No funds in watchlist to snapshot.")
            return
        
        logger.info(f"üì∏ Starting 15:00 Valuation Snapshot for {len(codes)} funds...")
        
        # We can use batch valuation for speed
        valuations = self.calculate_batch_valuation(codes)
        
        trade_date = datetime.now().date()
        
        count = 0
        for val in valuations:
            if 'error' in val: continue
            
            self.db.save_valuation_snapshot(
                trade_date=trade_date,
                fund_code=val['fund_code'],
                est_growth=val['estimated_growth'],
                components_json=val['components']
            )
            count += 1
            
        logger.info(f"‚úÖ Successfully archived {count} snapshots for {trade_date}")

    def reconcile_official_valuations(self, trade_date=None):
        """
        Fetch real growth for specific funds in the archive by looking up 
        their historical NAV series. Targeted and precise.
        """
        if trade_date is None:
            # Default to the most recent archive date that hasn't been reconciled
            trade_date = datetime.now().date()
            
        # 1. Get the list of funds that need reconciliation for this date
        conn = self.db.get_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT fund_code FROM fund_valuation_archive 
                    WHERE trade_date = %s AND official_growth IS NULL
                """, (trade_date,))
                codes = [row[0] for row in cursor.fetchall()]
        finally:
            conn.close()
            
        if not codes:
            logger.info(f"No pending reconciliation found for {trade_date}")
            return

        logger.info(f"‚öñÔ∏è Starting Targeted Reconciliation for {len(codes)} funds on {trade_date}...")
        
        count = 0
        for code in codes:
            try:
                # 2. Fetch the historical NAV series for this specific fund
                # This is more robust than a daily dump as it allows historical backfilling
                df = ak.fund_open_fund_info_em(symbol=code, indicator="Âçï‰ΩçÂáÄÂÄºËµ∞Âäø")
                
                if df.empty:
                    logger.warning(f"No NAV history found for {code}")
                    continue
                
                # df usually has columns: ['ÂáÄÂÄºÊó•Êúü', 'Âçï‰ΩçÂáÄÂÄº', 'Êó•Â¢ûÈïøÁéá', ...]
                # Convert 'ÂáÄÂÄºÊó•Êúü' to date objects for comparison
                df['ÂáÄÂÄºÊó•Êúü'] = pd.to_datetime(df['ÂáÄÂÄºÊó•Êúü']).dt.date
                
                # 3. Find the record matching our trade_date
                match = df[df['ÂáÄÂÄºÊó•Êúü'] == trade_date]
                
                if not match.empty:
                    # 'Êó•Â¢ûÈïøÁéá' is usually a string like "1.23" or "0.00"
                    official_growth = float(match.iloc[0]['Êó•Â¢ûÈïøÁéá'])
                    
                    # 4. Update the archive with the official value and trigger grading
                    self.db.update_official_nav(trade_date, code, official_growth)
                    count += 1
                    logger.info(f"üéØ Matched {code}: Est vs Official applied.")
                else:
                    logger.info(f"‚è≥ NAV for {code} on {trade_date} not yet released by fund company.")
                    
            except Exception as e:
                logger.error(f"Failed to reconcile {code}: {e}")
            
        logger.info(f"‚ú® Reconciliation session finished. {count}/{len(codes)} funds updated.")
